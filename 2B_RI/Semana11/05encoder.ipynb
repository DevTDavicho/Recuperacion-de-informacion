{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definir el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\n",
    "    \"el gato duerme tranquilamente en el sofá mientras observa por la ventana\",\n",
    "    \"los perros corren rápidamente por el parque persiguiendo una pelota amarilla\",\n",
    "    \"el jardín tiene flores de múltiples colores y árboles altos que dan sombra\",\n",
    "    \"en el bosque, los pájaros cantan y los ciervos pastan cerca del río cristalino\",\n",
    "    \"el gato y el perro comparten la casa, pero a veces pelean por la comida\",\n",
    "    \"el estanque del jardín tiene peces de colores brillantes y ranas que croan por la noche\",\n",
    "    \"en la ciudad, las personas caminan rápido mientras los autos avanzan lentamente por el tráfico\",\n",
    "    \"la biblioteca está llena de libros antiguos y mesas de estudio donde los estudiantes leen en silencio\",\n",
    "    \"en la playa, las olas golpean suavemente la arena y los niños construyen castillos mientras los adultos toman el sol\",\n",
    "    \"el laboratorio tiene computadoras avanzadas y científicos trabajando en experimentos complejos\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generar la matriz Término-Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario:\n",
      "['adultos' 'altos' 'amarilla' 'antiguos' 'arena' 'autos' 'avanzadas'\n",
      " 'avanzan' 'biblioteca' 'bosque' 'brillantes' 'caminan' 'cantan' 'casa'\n",
      " 'castillos' 'cerca' 'científicos' 'ciervos' 'ciudad' 'colores' 'comida'\n",
      " 'comparten' 'complejos' 'computadoras' 'construyen' 'corren' 'cristalino'\n",
      " 'croan' 'dan' 'duerme' 'estanque' 'estudiantes' 'estudio' 'experimentos'\n",
      " 'flores' 'gato' 'golpean' 'jardín' 'laboratorio' 'leen' 'lentamente'\n",
      " 'libros' 'llena' 'mesas' 'mientras' 'múltiples' 'niños' 'noche' 'observa'\n",
      " 'olas' 'parque' 'pastan' 'peces' 'pelean' 'pelota' 'perro' 'perros'\n",
      " 'persiguiendo' 'personas' 'playa' 'pájaros' 'ranas' 'rápidamente'\n",
      " 'rápido' 'río' 'silencio' 'sofá' 'sol' 'sombra' 'suavemente' 'toman'\n",
      " 'trabajando' 'tranquilamente' 'tráfico' 'veces' 'ventana' 'árboles']\n",
      "\n",
      "Matriz Término-Documento (X):\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.37796447\n",
      "  0.         0.         0.         0.         0.         0.37796447\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.37796447 0.        ]\n",
      " [0.         0.         0.37796447 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.\n",
      "  0.37796447 0.         0.37796447 0.37796447 0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35355339 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35355339 0.         0.\n",
      "  0.35355339 0.         0.         0.35355339 0.         0.35355339\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35355339 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35355339 0.         0.         0.         0.35355339 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.         0.\n",
      "  0.         0.         0.37796447 0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.37796447\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.37796447\n",
      "  0.         0.37796447 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37796447 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35355339 0.         0.\n",
      "  0.35355339 0.         0.         0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.         0.33333333\n",
      "  0.33333333 0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.28867513 0.         0.         0.         0.28867513 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28867513 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28867513 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28867513 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28867513 0.         0.28867513 0.\n",
      "  0.         0.28867513 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28867513\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28867513 0.         0.28867513 0.28867513 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37796447 0.\n",
      "  0.         0.         0.         0.         0.37796447 0.37796447\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37796447 0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.37796447\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "X = X / np.linalg.norm(X, axis=1, keepdims=True)  # Normalización fila por fila\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Mostrar la matriz Término-Documento\n",
    "print(\"Vocabulario:\")\n",
    "print(vocab)\n",
    "print(\"\\nMatriz Término-Documento (X):\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Calcular las dimensiones de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, d_model = X.shape  # Número de documentos y tamaño del vocabulario\n",
    "num_heads = 7  # Dividir en 3 cabezas\n",
    "d_k = d_model // num_heads  # Dimensión por cabeza (para simplificar)\n",
    "d_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensiones clave: X: (N, d_model), donde N = número de documentos, d_model = tamaño del vocabulario vocab: (d_model,), contiene los términos únicos del vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Crear las matrices entrenables (W_Q, W_K, W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n",
       "        0.15599452, 0.05808361, 0.86617615, 0.60111501, 0.70807258,\n",
       "        0.02058449],\n",
       "       [0.96990985, 0.83244264, 0.21233911, 0.18182497, 0.18340451,\n",
       "        0.30424224, 0.52475643, 0.43194502, 0.29122914, 0.61185289,\n",
       "        0.13949386],\n",
       "       [0.29214465, 0.36636184, 0.45606998, 0.78517596, 0.19967378,\n",
       "        0.51423444, 0.59241457, 0.04645041, 0.60754485, 0.17052412,\n",
       "        0.06505159],\n",
       "       [0.94888554, 0.96563203, 0.80839735, 0.30461377, 0.09767211,\n",
       "        0.68423303, 0.44015249, 0.12203823, 0.49517691, 0.03438852,\n",
       "        0.9093204 ],\n",
       "       [0.25877998, 0.66252228, 0.31171108, 0.52006802, 0.54671028,\n",
       "        0.18485446, 0.96958463, 0.77513282, 0.93949894, 0.89482735,\n",
       "        0.59789998],\n",
       "       [0.92187424, 0.0884925 , 0.19598286, 0.04522729, 0.32533033,\n",
       "        0.38867729, 0.27134903, 0.82873751, 0.35675333, 0.28093451,\n",
       "        0.54269608],\n",
       "       [0.14092422, 0.80219698, 0.07455064, 0.98688694, 0.77224477,\n",
       "        0.19871568, 0.00552212, 0.81546143, 0.70685734, 0.72900717,\n",
       "        0.77127035],\n",
       "       [0.07404465, 0.35846573, 0.11586906, 0.86310343, 0.62329813,\n",
       "        0.33089802, 0.06355835, 0.31098232, 0.32518332, 0.72960618,\n",
       "        0.63755747],\n",
       "       [0.88721274, 0.47221493, 0.11959425, 0.71324479, 0.76078505,\n",
       "        0.5612772 , 0.77096718, 0.4937956 , 0.52273283, 0.42754102,\n",
       "        0.02541913],\n",
       "       [0.10789143, 0.03142919, 0.63641041, 0.31435598, 0.50857069,\n",
       "        0.90756647, 0.24929223, 0.41038292, 0.75555114, 0.22879817,\n",
       "        0.07697991],\n",
       "       [0.28975145, 0.16122129, 0.92969765, 0.80812038, 0.63340376,\n",
       "        0.87146059, 0.80367208, 0.18657006, 0.892559  , 0.53934224,\n",
       "        0.80744016],\n",
       "       [0.8960913 , 0.31800347, 0.11005192, 0.22793516, 0.42710779,\n",
       "        0.81801477, 0.86073058, 0.00695213, 0.5107473 , 0.417411  ,\n",
       "        0.22210781],\n",
       "       [0.11986537, 0.33761517, 0.9429097 , 0.32320293, 0.51879062,\n",
       "        0.70301896, 0.3636296 , 0.97178208, 0.96244729, 0.2517823 ,\n",
       "        0.49724851],\n",
       "       [0.30087831, 0.28484049, 0.03688695, 0.60956433, 0.50267902,\n",
       "        0.05147875, 0.27864646, 0.90826589, 0.23956189, 0.14489487,\n",
       "        0.48945276],\n",
       "       [0.98565045, 0.24205527, 0.67213555, 0.76161962, 0.23763754,\n",
       "        0.72821635, 0.36778313, 0.63230583, 0.63352971, 0.53577468,\n",
       "        0.09028977],\n",
       "       [0.8353025 , 0.32078006, 0.18651851, 0.04077514, 0.59089294,\n",
       "        0.67756436, 0.01658783, 0.51209306, 0.22649578, 0.64517279,\n",
       "        0.17436643],\n",
       "       [0.69093774, 0.38673535, 0.93672999, 0.13752094, 0.34106635,\n",
       "        0.11347352, 0.92469362, 0.87733935, 0.25794163, 0.65998405,\n",
       "        0.8172222 ],\n",
       "       [0.55520081, 0.52965058, 0.24185229, 0.09310277, 0.89721576,\n",
       "        0.90041806, 0.63310146, 0.33902979, 0.34920957, 0.72595568,\n",
       "        0.89711026],\n",
       "       [0.88708642, 0.77987555, 0.64203165, 0.08413996, 0.16162871,\n",
       "        0.89855419, 0.60642906, 0.00919705, 0.10147154, 0.66350177,\n",
       "        0.00506158],\n",
       "       [0.16080805, 0.54873379, 0.6918952 , 0.65196126, 0.22426931,\n",
       "        0.71217922, 0.23724909, 0.3253997 , 0.74649141, 0.6496329 ,\n",
       "        0.84922341],\n",
       "       [0.65761289, 0.5683086 , 0.09367477, 0.3677158 , 0.26520237,\n",
       "        0.24398964, 0.97301055, 0.39309772, 0.89204656, 0.63113863,\n",
       "        0.7948113 ],\n",
       "       [0.50263709, 0.57690388, 0.49251769, 0.19524299, 0.72245212,\n",
       "        0.28077236, 0.02431597, 0.6454723 , 0.17711068, 0.94045858,\n",
       "        0.95392858],\n",
       "       [0.91486439, 0.3701587 , 0.01545662, 0.92831856, 0.42818415,\n",
       "        0.96665482, 0.96361998, 0.85300946, 0.29444889, 0.38509773,\n",
       "        0.85113667],\n",
       "       [0.31692201, 0.16949275, 0.55680126, 0.93615477, 0.6960298 ,\n",
       "        0.57006117, 0.09717649, 0.61500723, 0.99005385, 0.14008402,\n",
       "        0.51832965],\n",
       "       [0.87737307, 0.74076862, 0.69701574, 0.70248408, 0.35949115,\n",
       "        0.29359184, 0.80936116, 0.81011339, 0.86707232, 0.91324055,\n",
       "        0.5113424 ],\n",
       "       [0.50151629, 0.79829518, 0.64996393, 0.70196688, 0.79579267,\n",
       "        0.89000534, 0.33799516, 0.37558295, 0.09398194, 0.57828014,\n",
       "        0.03594227],\n",
       "       [0.46559802, 0.54264463, 0.28654125, 0.59083326, 0.03050025,\n",
       "        0.03734819, 0.82260056, 0.36019064, 0.12706051, 0.52224326,\n",
       "        0.76999355],\n",
       "       [0.21582103, 0.62289048, 0.08534746, 0.05168172, 0.53135463,\n",
       "        0.54063512, 0.6374299 , 0.72609133, 0.97585208, 0.51630035,\n",
       "        0.32295647],\n",
       "       [0.79518619, 0.27083225, 0.43897142, 0.07845638, 0.02535074,\n",
       "        0.96264841, 0.83598012, 0.69597421, 0.40895294, 0.17329432,\n",
       "        0.15643704],\n",
       "       [0.2502429 , 0.54922666, 0.71459592, 0.66019738, 0.2799339 ,\n",
       "        0.95486528, 0.73789692, 0.55435405, 0.61172075, 0.41960006,\n",
       "        0.24773099],\n",
       "       [0.35597268, 0.75784611, 0.01439349, 0.11607264, 0.04600264,\n",
       "        0.0407288 , 0.85546058, 0.70365786, 0.47417383, 0.09783416,\n",
       "        0.49161588],\n",
       "       [0.47347177, 0.17320187, 0.43385165, 0.39850473, 0.6158501 ,\n",
       "        0.63509365, 0.04530401, 0.37461261, 0.62585992, 0.50313626,\n",
       "        0.85648984],\n",
       "       [0.65869363, 0.16293443, 0.07056875, 0.64241928, 0.02651131,\n",
       "        0.58577558, 0.94023024, 0.57547418, 0.38816993, 0.64328822,\n",
       "        0.45825289],\n",
       "       [0.54561679, 0.94146481, 0.38610264, 0.96119056, 0.90535064,\n",
       "        0.19579113, 0.0693613 , 0.100778  , 0.01822183, 0.09444296,\n",
       "        0.68300677],\n",
       "       [0.07118865, 0.31897563, 0.84487531, 0.02327194, 0.81446848,\n",
       "        0.28185477, 0.11816483, 0.69673717, 0.62894285, 0.87747201,\n",
       "        0.73507104],\n",
       "       [0.80348093, 0.28203457, 0.17743954, 0.75061475, 0.80683474,\n",
       "        0.99050514, 0.41261768, 0.37201809, 0.77641296, 0.34080354,\n",
       "        0.93075733],\n",
       "       [0.85841275, 0.42899403, 0.75087107, 0.75454287, 0.10312387,\n",
       "        0.90255291, 0.50525237, 0.82645747, 0.3200496 , 0.89552323,\n",
       "        0.38920168],\n",
       "       [0.01083765, 0.90538198, 0.09128668, 0.31931364, 0.95006197,\n",
       "        0.95060715, 0.57343789, 0.63183721, 0.44844552, 0.29321077,\n",
       "        0.32866455],\n",
       "       [0.67251846, 0.75237453, 0.79157904, 0.78961814, 0.0912061 ,\n",
       "        0.4944203 , 0.05755876, 0.54952888, 0.4415305 , 0.88770418,\n",
       "        0.35091501],\n",
       "       [0.11706702, 0.14299168, 0.76151063, 0.61821806, 0.10112268,\n",
       "        0.08410681, 0.70096913, 0.07276301, 0.82186006, 0.70624223,\n",
       "        0.08134878],\n",
       "       [0.08483771, 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n",
       "        0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072,\n",
       "        0.77714692],\n",
       "       [0.55840425, 0.42422201, 0.90635439, 0.11119748, 0.4926251 ,\n",
       "        0.01135364, 0.46866064, 0.05630328, 0.11881792, 0.11752625,\n",
       "        0.6492103 ],\n",
       "       [0.74604488, 0.58336877, 0.96217255, 0.37487058, 0.28571209,\n",
       "        0.86859913, 0.22359584, 0.96322254, 0.01215447, 0.96987883,\n",
       "        0.04315991],\n",
       "       [0.89114311, 0.52770111, 0.9929648 , 0.07379656, 0.55385428,\n",
       "        0.96930254, 0.52309784, 0.62939864, 0.69574869, 0.45454106,\n",
       "        0.62755808],\n",
       "       [0.58431431, 0.90115801, 0.04544638, 0.28096319, 0.95041148,\n",
       "        0.89026378, 0.45565675, 0.6201326 , 0.27738118, 0.18812116,\n",
       "        0.4636984 ],\n",
       "       [0.35335223, 0.58365611, 0.07773464, 0.97439481, 0.98621074,\n",
       "        0.69816171, 0.53609637, 0.30952762, 0.81379502, 0.68473117,\n",
       "        0.16261694],\n",
       "       [0.91092718, 0.82253724, 0.94979991, 0.72571951, 0.6134152 ,\n",
       "        0.41824304, 0.93272848, 0.86606389, 0.04521867, 0.02636697,\n",
       "        0.37646337],\n",
       "       [0.81055333, 0.98727613, 0.15041689, 0.59413072, 0.38089086,\n",
       "        0.9699144 , 0.84211892, 0.8383287 , 0.46869316, 0.4148195 ,\n",
       "        0.27340707],\n",
       "       [0.0563755 , 0.86472238, 0.81290101, 0.99971767, 0.99663684,\n",
       "        0.55543171, 0.76898742, 0.94476573, 0.84964739, 0.2473481 ,\n",
       "        0.45054414],\n",
       "       [0.12915942, 0.95405103, 0.60617463, 0.22864281, 0.67170068,\n",
       "        0.61812824, 0.35816272, 0.11355759, 0.6715732 , 0.5203077 ,\n",
       "        0.77231839],\n",
       "       [0.5201635 , 0.8521815 , 0.55190684, 0.56093797, 0.8766536 ,\n",
       "        0.40348287, 0.13401523, 0.02878268, 0.75513726, 0.62030955,\n",
       "        0.70407977],\n",
       "       [0.21296416, 0.13637148, 0.01454467, 0.35058756, 0.58991769,\n",
       "        0.39224405, 0.43747492, 0.90415869, 0.34825547, 0.51398949,\n",
       "        0.78365301],\n",
       "       [0.39654278, 0.6220867 , 0.86236371, 0.94952062, 0.14707348,\n",
       "        0.92658763, 0.49211629, 0.25824439, 0.45913576, 0.98003258,\n",
       "        0.49261809],\n",
       "       [0.32875161, 0.63340085, 0.24014562, 0.07586333, 0.12887972,\n",
       "        0.12804584, 0.15190269, 0.13882717, 0.64087474, 0.18188008,\n",
       "        0.34566728],\n",
       "       [0.89678841, 0.47396164, 0.66755774, 0.17231987, 0.19228902,\n",
       "        0.04086862, 0.16893506, 0.27859034, 0.17701048, 0.08870253,\n",
       "        0.12063587],\n",
       "       [0.46077877, 0.20633372, 0.36426986, 0.50341727, 0.69039483,\n",
       "        0.03931214, 0.7994104 , 0.62790039, 0.08175903, 0.87357862,\n",
       "        0.9208724 ],\n",
       "       [0.06107796, 0.27687765, 0.80620128, 0.74825969, 0.18452102,\n",
       "        0.20934932, 0.3704721 , 0.48452299, 0.61825477, 0.36891364,\n",
       "        0.46253472],\n",
       "       [0.74747094, 0.0366832 , 0.25243694, 0.71334959, 0.89520684,\n",
       "        0.51167744, 0.53211349, 0.10717201, 0.44741237, 0.53261727,\n",
       "        0.2424705 ],\n",
       "       [0.26924323, 0.37728416, 0.0200712 , 0.32207917, 0.21144801,\n",
       "        0.32749735, 0.11976213, 0.89052728, 0.59359245, 0.67910232,\n",
       "        0.78917124],\n",
       "       [0.4984422 , 0.08692029, 0.53710654, 0.58684112, 0.74543947,\n",
       "        0.43165955, 0.1275803 , 0.28377591, 0.3630823 , 0.64591724,\n",
       "        0.5707783 ],\n",
       "       [0.35609673, 0.98651525, 0.60577482, 0.23722679, 0.10178247,\n",
       "        0.15285914, 0.24595773, 0.16068137, 0.18656702, 0.28509517,\n",
       "        0.1733736 ],\n",
       "       [0.89676542, 0.08023375, 0.52451139, 0.41039683, 0.98237862,\n",
       "        0.1120389 , 0.3978556 , 0.96947043, 0.86550713, 0.81707207,\n",
       "        0.25790283],\n",
       "       [0.17088759, 0.66864322, 0.92937599, 0.55676289, 0.57161269,\n",
       "        0.27997909, 0.76949293, 0.18704375, 0.32367924, 0.42543644,\n",
       "        0.50761038],\n",
       "       [0.24240973, 0.11483682, 0.61062004, 0.28863055, 0.58123822,\n",
       "        0.15436272, 0.4811401 , 0.53258943, 0.05182354, 0.33660428,\n",
       "        0.13441468],\n",
       "       [0.06337497, 0.98996023, 0.32235384, 0.80987445, 0.25464065,\n",
       "        0.68150272, 0.76022786, 0.59563874, 0.47157619, 0.41184091,\n",
       "        0.34886827],\n",
       "       [0.92952914, 0.83061941, 0.96502691, 0.12429722, 0.73086748,\n",
       "        0.93834046, 0.18123307, 0.06649627, 0.74112065, 0.57447311,\n",
       "        0.84182878],\n",
       "       [0.13977238, 0.79526731, 0.20162732, 0.16365594, 0.1642658 ,\n",
       "        0.81457472, 0.66519722, 0.52306542, 0.35883048, 0.87720054,\n",
       "        0.39244511],\n",
       "       [0.81659944, 0.43913491, 0.37694443, 0.46267979, 0.30137787,\n",
       "        0.74760938, 0.50272039, 0.2322127 , 0.89957457, 0.38389122,\n",
       "        0.54355286],\n",
       "       [0.90647211, 0.624238  , 0.11689804, 0.93983212, 0.62770805,\n",
       "        0.33490561, 0.13927207, 0.79402519, 0.62007276, 0.53346109,\n",
       "        0.89389258],\n",
       "       [0.78859721, 0.15167488, 0.31172207, 0.24848914, 0.74394629,\n",
       "        0.03353243, 0.56988968, 0.76245869, 0.87676564, 0.34208175,\n",
       "        0.8212573 ],\n",
       "       [0.11063174, 0.84645229, 0.12748866, 0.39728729, 0.79729537,\n",
       "        0.14991743, 0.2292514 , 0.72225257, 0.72003654, 0.64114763,\n",
       "        0.69394844],\n",
       "       [0.54272444, 0.25179906, 0.34569599, 0.18159772, 0.90845056,\n",
       "        0.58339179, 0.40085142, 0.4620058 , 0.94728334, 0.1533514 ,\n",
       "        0.58622983],\n",
       "       [0.50588868, 0.61145424, 0.01811018, 0.87212391, 0.93211828,\n",
       "        0.56513318, 0.69665082, 0.92249938, 0.70723863, 0.15253904,\n",
       "        0.57628836],\n",
       "       [0.60671505, 0.42413067, 0.73644424, 0.93436701, 0.92556851,\n",
       "        0.45083937, 0.11323805, 0.9848412 , 0.83889809, 0.12466268,\n",
       "        0.92084188],\n",
       "       [0.86989636, 0.51883806, 0.59127544, 0.3990027 , 0.05476164,\n",
       "        0.33519724, 0.80285345, 0.00463202, 0.33349917, 0.39816869,\n",
       "        0.5373956 ],\n",
       "       [0.91985562, 0.34634599, 0.3469532 , 0.73750125, 0.45221794,\n",
       "        0.22460482, 0.45243952, 0.14085702, 0.17638699, 0.49836777,\n",
       "        0.41892545],\n",
       "       [0.9148459 , 0.3623939 , 0.58058835, 0.63226429, 0.01309446,\n",
       "        0.66353737, 0.17803597, 0.96107032, 0.14866273, 0.41462412,\n",
       "        0.08534967]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)  # Para reproducibilidad\n",
    "W_Q = np.random.rand(d_model, d_k)\n",
    "W_K = np.random.rand(d_model, d_k)\n",
    "W_V = np.random.rand(d_model, d_k)\n",
    "W_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calcular Q, K y V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23213784, 1.64422451, 0.87577149, 1.68752599, 1.73199157,\n",
       "        1.88807565, 1.58346187, 1.54122282, 1.42024625, 1.02956775,\n",
       "        1.31546369],\n",
       "       [1.20572532, 1.31267221, 1.63035456, 1.60210555, 1.40442135,\n",
       "        1.07704647, 1.09815255, 0.57002528, 1.1425945 , 1.0525493 ,\n",
       "        0.80821092],\n",
       "       [1.47877264, 1.5721297 , 1.07996021, 1.34396936, 1.35218908,\n",
       "        1.7352883 , 1.11121575, 1.71350231, 1.45189965, 1.49845801,\n",
       "        1.1846687 ],\n",
       "       [0.96035495, 1.37000758, 1.14441891, 0.97579282, 1.23471842,\n",
       "        1.57420423, 1.24764473, 1.50400103, 1.21168509, 1.26744569,\n",
       "        1.31578201],\n",
       "       [1.48314619, 1.16060046, 0.75449641, 1.09663413, 1.19860261,\n",
       "        0.78212231, 1.30123991, 1.16799095, 1.18728658, 1.32700417,\n",
       "        1.87957395],\n",
       "       [1.10911551, 1.65663459, 1.18437291, 1.37928171, 1.37724434,\n",
       "        1.81166125, 1.71096519, 1.6403462 , 1.88474288, 1.52319448,\n",
       "        1.35192751],\n",
       "       [1.52220555, 1.44962883, 0.95026271, 1.1390293 , 1.67294358,\n",
       "        1.73545202, 1.31928837, 1.64577924, 1.14403678, 1.16781487,\n",
       "        1.49756536],\n",
       "       [2.0701507 , 1.42762874, 2.00681375, 1.12038749, 1.22166673,\n",
       "        1.77936068, 1.43140348, 1.11803478, 1.47388046, 1.47700517,\n",
       "        1.49752937],\n",
       "       [2.07656376, 2.08625034, 1.76623283, 1.80941456, 1.79745531,\n",
       "        1.60346449, 1.69915761, 2.1681349 , 2.08276156, 1.93275857,\n",
       "        1.68913503],\n",
       "       [1.44552817, 1.38872545, 1.17430394, 1.86007189, 1.56573006,\n",
       "        1.18019725, 0.95201075, 1.61509139, 1.38196563, 1.15266748,\n",
       "        1.73036312]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.dot(X, W_Q)\n",
    "K = np.dot(X, W_K)\n",
    "V = np.dot(X, W_V)\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cálculo de la atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05709171, 0.0552337 , 0.06278384, 0.09143275, 0.06198191,\n",
       "        0.09851097, 0.06102197, 0.087238  , 0.34945684, 0.07524832],\n",
       "       [0.07154565, 0.06310519, 0.07645364, 0.08113967, 0.06335461,\n",
       "        0.10178146, 0.06367481, 0.07523307, 0.32719086, 0.07652104],\n",
       "       [0.06618297, 0.05826267, 0.06909248, 0.09963156, 0.06135958,\n",
       "        0.09882278, 0.06132792, 0.08834127, 0.3279827 , 0.06899607],\n",
       "       [0.06876649, 0.05864783, 0.06905512, 0.10461064, 0.06734636,\n",
       "        0.09927878, 0.06403948, 0.09319846, 0.29863506, 0.07642179],\n",
       "       [0.08276039, 0.05700517, 0.06433089, 0.09247222, 0.07602703,\n",
       "        0.0876793 , 0.0639315 , 0.08576227, 0.32112291, 0.06890832],\n",
       "       [0.06012896, 0.05744295, 0.06162869, 0.09367959, 0.06434869,\n",
       "        0.10931348, 0.05666522, 0.09217708, 0.33685357, 0.06776176],\n",
       "       [0.06820141, 0.05070162, 0.06661153, 0.09588033, 0.06088958,\n",
       "        0.08808908, 0.06092915, 0.08800833, 0.34973763, 0.07095135],\n",
       "       [0.07076776, 0.04593253, 0.06896902, 0.08168918, 0.04845598,\n",
       "        0.08809808, 0.04778775, 0.06547911, 0.43284967, 0.04997091],\n",
       "       [0.05346882, 0.04375735, 0.0514907 , 0.08058698, 0.04870579,\n",
       "        0.08171974, 0.04299106, 0.07314038, 0.47228086, 0.05185832],\n",
       "       [0.06259566, 0.05577309, 0.06403658, 0.09672448, 0.06456649,\n",
       "        0.08179952, 0.06510329, 0.08124477, 0.3536357 , 0.07452043]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Producto QK^T\n",
    "scores = np.dot(Q, K.T)\n",
    "\n",
    "# Escalar por raíz de d_k\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "# Softmax para obtener los pesos\n",
    "attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=1, keepdims=True)\n",
    "\n",
    "# Ponderar V con los pesos\n",
    "output = np.dot(attention_weights, V)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.64206137, 1.67911209, 1.28388025, 1.46001905, 1.43233938,\n",
       "        1.51001449, 1.4178411 , 1.78240841, 1.58570075, 1.54985623,\n",
       "        1.79769982],\n",
       "       [1.61721569, 1.65642354, 1.27784558, 1.45715216, 1.41812947,\n",
       "        1.50145816, 1.4053051 , 1.75505027, 1.58275401, 1.53691896,\n",
       "        1.78060973],\n",
       "       [1.62381626, 1.66542711, 1.28419872, 1.45155106, 1.41998214,\n",
       "        1.49981291, 1.40704485, 1.76208094, 1.58119692, 1.53739356,\n",
       "        1.78089676],\n",
       "       [1.59842606, 1.65419835, 1.28445576, 1.43479749, 1.39856871,\n",
       "        1.49036574, 1.40056302, 1.73807779, 1.57251708, 1.51835984,\n",
       "        1.75424582],\n",
       "       [1.6175553 , 1.66223605, 1.2672905 , 1.44091042, 1.41337983,\n",
       "        1.49577708, 1.40517965, 1.75006971, 1.57897013, 1.53321474,\n",
       "        1.77727529],\n",
       "       [1.62459938, 1.6691575 , 1.28465532, 1.46372383, 1.4240292 ,\n",
       "        1.50881087, 1.40990085, 1.76671049, 1.57927142, 1.5472827 ,\n",
       "        1.78033189],\n",
       "       [1.64619558, 1.67852951, 1.2782157 , 1.45178735, 1.43315669,\n",
       "        1.50633086, 1.41740565, 1.78421548, 1.59037121, 1.55058306,\n",
       "        1.80543611],\n",
       "       [1.71304088, 1.70228369, 1.27079387, 1.49411496, 1.49184731,\n",
       "        1.53081171, 1.43485889, 1.84910145, 1.61682875, 1.60720194,\n",
       "        1.8823649 ],\n",
       "       [1.74863385, 1.73213703, 1.27313369, 1.50514222, 1.51949859,\n",
       "        1.54438855, 1.45167572, 1.88588913, 1.62009996, 1.62976113,\n",
       "        1.9134446 ],\n",
       "       [1.65416031, 1.68468465, 1.27885981, 1.44909272, 1.43745995,\n",
       "        1.50310032, 1.41943535, 1.78701   , 1.58826809, 1.54621742,\n",
       "        1.8098719 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Definir Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Evitar desbordamiento numérico\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Función de Self-Attention\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]  # Dimensión de las claves (d_k)\n",
    "    # 1. Producto QK^T (relaciones entre consultas y claves)\n",
    "    scores = np.dot(Q, K.T)  # Dimensión: (N, N)\n",
    "    # 2. Escalamiento por sqrt(d_k)\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    # 3. Aplicar softmax (normalización de las relaciones)\n",
    "    attention_weights = softmax(scaled_scores)  # Dimensión: (N, N)\n",
    "    # 4. Multiplicar por V (valores ponderados por atención)\n",
    "    output = np.dot(attention_weights, V)  # Dimensión: (N, d_k)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Implementar Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de Multi-Head Attention\n",
    "def multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads):\n",
    "    \"\"\"\n",
    "    X: Matriz de entrada (N, d_model)\n",
    "    W_Q, W_K, W_V: Matrices entrenables para Q, K, V (d_model, d_model)\n",
    "    W_O: Matriz para la proyección final (d_model, d_model)\n",
    "    num_heads: Número de cabezas de atención\n",
    "    \"\"\"\n",
    "    N, d_model = X.shape\n",
    "    assert d_model % num_heads == 0, \"El número de cabezas debe dividir exactamente d_model\"\n",
    "    d_k = d_model // num_heads  # Dimensión por cabeza\n",
    "\n",
    "    # Dividir las matrices de proyección para cada cabeza\n",
    "    heads_output = []  # Lista para almacenar la salida de cada cabeza\n",
    "    all_attention_weights = []  # Lista para almacenar los pesos de atención de cada cabeza\n",
    "    for head in range(num_heads):\n",
    "        # Proyectar las entradas en Q, K, V para esta cabeza\n",
    "        Q = np.dot(X, W_Q[:, head * d_k:(head + 1) * d_k])  # Dimensión: (N, d_k)\n",
    "        K = np.dot(X, W_K[:, head * d_k:(head + 1) * d_k])  # Dimensión: (N, d_k)\n",
    "        V = np.dot(X, W_V[:, head * d_k:(head + 1) * d_k])  # Dimensión: (N, d_k)\n",
    "\n",
    "        # Calcular Self-Attention para esta cabeza\n",
    "        head_output, attention_weights = self_attention(Q, K, V)  # Dimensión: (N, d_k)\n",
    "        heads_output.append(head_output)\n",
    "        all_attention_weights.append(attention_weights)\n",
    "\n",
    "    # Concatenar las salidas de todas las cabezas\n",
    "    concatenated = np.concatenate(heads_output, axis=1)  # Dimensión: (N, d_model)\n",
    "\n",
    "    # Aplicar proyección final\n",
    "    output = np.dot(concatenated, W_O[:, :d_model])  # Aseguramos que W_O tenga la forma correcta\n",
    "    return output, all_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualizar pesos de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(weights, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights, annot=True, cmap=\"Blues\", fmt=\".2f\", cbar=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Keys (K)\")\n",
    "    plt.ylabel(\"Queries (Q)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Definir parámetros entrenables aleatorios para Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "d_model = X.shape[1]  # Tamaño del vocabulario\n",
    "num_heads = 7\n",
    "print(d_model)\n",
    "print(num_heads)\n",
    "assert d_model % num_heads == 0, \"El número de cabezas debe dividir exactamente d_model\"\n",
    "d_k = d_model // num_heads  # Dimensión por cabeza (d_model dividido entre num_heads)\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "W_Q = np.random.rand(d_model, d_model)  # Proyección para Q\n",
    "np.random.seed(43)  # Para reproducibilidad\n",
    "W_K = np.random.rand(d_model, d_model)  # Proyección para K\n",
    "np.random.seed(44)  # Para reproducibilidad\n",
    "W_V = np.random.rand(d_model, d_model)  # Proyección para V\n",
    "np.random.seed(45)  # Para reproducibilidad\n",
    "W_O = np.random.rand(d_model, d_model)  # Proyección final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Calcular la salida y pesos de atención para Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proyectar X para generar Q, K, V usando matrices entrenables\n",
    "Q = np.dot(X, W_Q[:, :d_k])  # Proyección para Q (dimensión: (N, d_k))\n",
    "K = np.dot(X, W_K[:, :d_k])  # Proyección para K (dimensión: (N, d_k))\n",
    "V = np.dot(X, W_V[:, :d_k])  # Proyección para V (dimensión: (N, d_k))\n",
    "self_attention_output, self_attention_weights = self_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Calcular la salida y pesos de atención para Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_output, multi_head_attention_weights = multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Mostrar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: ['adultos' 'altos' 'amarilla' 'antiguos' 'arena' 'autos' 'avanzadas'\n",
      " 'avanzan' 'biblioteca' 'bosque' 'brillantes' 'caminan' 'cantan' 'casa'\n",
      " 'castillos' 'cerca' 'científicos' 'ciervos' 'ciudad' 'colores' 'comida'\n",
      " 'comparten' 'complejos' 'computadoras' 'construyen' 'corren' 'cristalino'\n",
      " 'croan' 'dan' 'duerme' 'estanque' 'estudiantes' 'estudio' 'experimentos'\n",
      " 'flores' 'gato' 'golpean' 'jardín' 'laboratorio' 'leen' 'lentamente'\n",
      " 'libros' 'llena' 'mesas' 'mientras' 'múltiples' 'niños' 'noche' 'observa'\n",
      " 'olas' 'parque' 'pastan' 'peces' 'pelean' 'pelota' 'perro' 'perros'\n",
      " 'persiguiendo' 'personas' 'playa' 'pájaros' 'ranas' 'rápidamente'\n",
      " 'rápido' 'río' 'silencio' 'sofá' 'sol' 'sombra' 'suavemente' 'toman'\n",
      " 'trabajando' 'tranquilamente' 'tráfico' 'veces' 'ventana' 'árboles']\n",
      "\n",
      "Salida de Self-Attention:\n",
      " [[1.67497984 1.58827767 1.72805205 1.40604697 1.74370797 1.72293662\n",
      "  1.60901901 2.00507542 1.50836324 1.55318661 2.01428371]\n",
      " [1.66132367 1.572456   1.70959556 1.40424529 1.7144358  1.69981904\n",
      "  1.60347796 1.98126176 1.49593009 1.54894179 1.98778309]\n",
      " [1.70190824 1.60479103 1.76090868 1.40968988 1.78346617 1.75678838\n",
      "  1.62406107 2.03923013 1.52682746 1.56019182 2.057661  ]\n",
      " [1.66886178 1.584629   1.72278949 1.40673565 1.72920705 1.7149596\n",
      "  1.60662331 1.99883657 1.50194675 1.55055523 2.00127284]\n",
      " [1.70385658 1.60100117 1.75856386 1.40693659 1.78350379 1.75853427\n",
      "  1.61987653 2.03723612 1.52332705 1.55929698 2.05719544]\n",
      " [1.66913654 1.59104841 1.72958448 1.40718501 1.73625929 1.71816923\n",
      "  1.60892577 2.00308345 1.50690645 1.55312701 2.00786191]\n",
      " [1.74721771 1.63610416 1.81505799 1.41406612 1.84978466 1.8180162\n",
      "  1.64397016 2.09770645 1.55598744 1.5705185  2.12680618]\n",
      " [1.70120501 1.60017088 1.75817352 1.4105907  1.77329408 1.75124374\n",
      "  1.62445175 2.03319795 1.52307212 1.55872919 2.05148632]\n",
      " [1.79004195 1.66425405 1.86373903 1.41380486 1.91862595 1.8771924\n",
      "  1.66182073 2.1513945  1.58710582 1.58091122 2.19583551]\n",
      " [1.69999779 1.60680304 1.76137835 1.41149462 1.7740794  1.75375618\n",
      "  1.62306759 2.03620162 1.52419882 1.5595351  2.04992965]]\n",
      "\n",
      "Salida de Multi-Head Attention:\n",
      " [[63.93788977 62.31349294 58.54833773 54.75144379 53.68845184 63.52206178\n",
      "  57.64403342 54.83462825 54.81805295 60.0126368  62.23683423 54.86181235\n",
      "  60.61720774 57.35933642 59.9074829  62.30473377 53.29538553 60.42624195\n",
      "  64.40391952 55.83954873 55.0350811  56.08339699 56.91984603 60.42219514\n",
      "  55.8678096  62.52555849 58.40337169 60.44997192 60.00999383 54.14972222\n",
      "  54.25075855 59.87018988 62.97427882 58.2519915  61.41374874 60.98925846\n",
      "  58.67394862 49.71667571 62.5424738  60.75607277 63.12906965 52.43220168\n",
      "  61.90556476 58.4689945  55.21166488 49.69670957 56.31879246 56.03882464\n",
      "  55.86199636 61.45540271 65.81920937 63.40219983 61.45600471 60.808239\n",
      "  58.07833749 55.78466816 69.19915533 70.11232251 57.98104783 53.56583174\n",
      "  58.84071531 63.29344778 58.81463783 59.27012273 62.2412583  59.19096868\n",
      "  59.85513245 55.24028154 64.59762229 63.11648807 58.06523965 64.18242036\n",
      "  57.35087613 55.55373549 52.48330479 55.45421653 61.39012506]\n",
      " [64.10805683 62.54424377 58.67736074 54.95705081 53.77026214 63.76082599\n",
      "  57.80197034 55.00120712 54.99061189 60.20381557 62.37584175 55.01303569\n",
      "  60.73060567 57.448013   60.10039864 62.5498409  53.40410772 60.64140022\n",
      "  64.55922158 55.94605194 55.1200976  56.26144265 57.03858067 60.54861381\n",
      "  56.11038411 62.65959234 58.59644502 60.6027116  60.19213656 54.32505054\n",
      "  54.38097579 60.08032705 63.26713145 58.43526769 61.64093613 61.150155\n",
      "  58.9352335  49.99485076 62.71309949 60.93621183 63.31289141 52.62930802\n",
      "  62.06868613 58.62768876 55.37538122 49.83211277 56.40446264 56.13488673\n",
      "  55.99589012 61.52818688 65.94742974 63.53076484 61.60457607 60.94967272\n",
      "  58.24669055 55.82548266 69.36658233 70.32937474 58.04096953 53.75277259\n",
      "  58.96257104 63.54146051 58.93380044 59.41376917 62.41798609 59.34606287\n",
      "  60.07044667 55.418364   64.77354875 63.37291249 58.24552745 64.26293997\n",
      "  57.55950425 55.68874437 52.55061335 55.60117309 61.53767146]\n",
      " [64.38163003 62.72937724 58.92919281 55.12617444 54.09761437 63.97407588\n",
      "  58.03731158 55.25821588 55.18052453 60.45807333 62.65383888 55.26693779\n",
      "  61.06932824 57.70277855 60.37661637 62.77566123 53.67383395 60.88441259\n",
      "  64.85819057 56.19558549 55.41496138 56.42398854 57.30440215 60.88155408\n",
      "  56.29088958 62.96139571 58.85600575 60.8408042  60.42357018 54.51236577\n",
      "  54.63629594 60.28511496 63.45627564 58.63110686 61.80951942 61.3866659\n",
      "  59.13058864 50.07699125 62.95600046 61.18920776 63.59278864 52.81008716\n",
      "  62.29181667 58.85669901 55.59592104 50.05070526 56.69515964 56.40017055\n",
      "  56.27369643 61.85065189 66.26758019 63.818571   61.88344352 61.19149714\n",
      "  58.49290856 56.07464105 69.67871055 70.56283836 58.38949934 53.95440127\n",
      "  59.29652008 63.69409159 59.22792633 59.74535795 62.66869744 59.63808824\n",
      "  60.2749167  55.61005833 65.03379247 63.53987005 58.44031312 64.60702064\n",
      "  57.7628745  55.87621153 52.81367284 55.76793856 61.90053109]\n",
      " [64.23854992 62.66648431 58.83077873 55.02111013 53.91824381 63.84344788\n",
      "  57.90852879 55.10375312 55.12546598 60.3337597  62.54694724 55.10268659\n",
      "  60.91564223 57.52430053 60.21103257 62.64715779 53.56096484 60.74060462\n",
      "  64.67767721 56.068276   55.26222205 56.36909151 57.17910264 60.67528926\n",
      "  56.2303503  62.82892858 58.72148064 60.74751038 60.30967926 54.43986536\n",
      "  54.51024231 60.14739292 63.30772437 58.53522534 61.75317284 61.30585396\n",
      "  59.02993346 50.03452802 62.84337843 61.08496914 63.41356077 52.73373865\n",
      "  62.14814634 58.76195232 55.47195535 49.94011489 56.51698683 56.3239691\n",
      "  56.14713179 61.74659169 66.13509395 63.71046011 61.79422568 61.0755642\n",
      "  58.4132835  55.96963142 69.52698677 70.41280004 58.22510838 53.81613632\n",
      "  59.12755847 63.61802671 59.06696099 59.60505101 62.54241659 59.49383551\n",
      "  60.19822841 55.54095039 64.91562103 63.45519574 58.33891286 64.42765763\n",
      "  57.67880952 55.85192511 52.637722   55.68590707 61.69481869]\n",
      " [64.3336952  62.64171261 58.86240604 55.07135548 54.03404448 63.90617757\n",
      "  57.95519025 55.19189379 55.12883586 60.36139688 62.61652947 55.18585506\n",
      "  61.00774684 57.66067521 60.28921676 62.72818843 53.61488657 60.77514648\n",
      "  64.78854491 56.15085154 55.3244383  56.37723275 57.1982279  60.80239139\n",
      "  56.24522055 62.90351037 58.7763229  60.81084349 60.367309   54.42492077\n",
      "  54.59477097 60.15592536 63.37623464 58.57860929 61.76901365 61.28799003\n",
      "  59.01673079 49.98544086 62.90436638 61.08794206 63.51808398 52.76958185\n",
      "  62.23405941 58.82468818 55.5104865  50.03170223 56.58572349 56.37599349\n",
      "  56.21564992 61.82754823 66.20123045 63.78229325 61.82781871 61.15620207\n",
      "  58.4384874  56.05560671 69.62026044 70.44455036 58.29408612 53.86183026\n",
      "  59.22846545 63.66999874 59.16717276 59.67750655 62.5609965  59.61605979\n",
      "  60.1971938  55.57636226 64.97816526 63.48542451 58.33681968 64.56164968\n",
      "  57.72038603 55.85502258 52.74499118 55.69755998 61.8181512 ]\n",
      " [64.16662721 62.53878249 58.73212231 54.94508288 53.85203343 63.72869395\n",
      "  57.87187849 55.06033801 54.99121245 60.25536782 62.40712225 55.0325859\n",
      "  60.80651345 57.54632497 60.14955823 62.53686056 53.52063536 60.64510768\n",
      "  64.59671019 55.97538573 55.25598885 56.3133188  57.12008848 60.63373172\n",
      "  56.04927271 62.6948026  58.57944262 60.63971241 60.2031786  54.36705562\n",
      "  54.43219601 60.08829214 63.2278538  58.45559931 61.61740109 61.21216658\n",
      "  58.91186191 49.93979511 62.74586119 60.98133478 63.35493612 52.63542443\n",
      "  62.12801849 58.66674039 55.39666312 49.78956092 56.52721721 56.1974645\n",
      "  56.07571887 61.63539776 66.01578241 63.60548198 61.67217642 61.02352582\n",
      "  58.2711575  55.93612159 69.39679106 70.37159198 58.1667414  53.76249874\n",
      "  59.0397263  63.48384586 58.97667986 59.5046661  62.46984998 59.36978794\n",
      "  60.06830772 55.41717736 64.78436584 63.34212878 58.3201333  64.3266043\n",
      "  57.51537106 55.71530766 52.6472878  55.60816376 61.57188351]\n",
      " [64.67702737 62.87086516 59.07859793 55.34177252 54.34656741 64.22488572\n",
      "  58.23561256 55.51592341 55.35751792 60.60063037 62.90791828 55.48820267\n",
      "  61.32696158 58.00582586 60.55026076 62.99837377 53.89530164 61.05739057\n",
      "  65.10703089 56.46592766 55.57707444 56.58319622 57.4249243  61.13669114\n",
      "  56.45219831 63.18309183 59.08867581 61.10202069 60.6213001  54.61496152\n",
      "  54.89591013 60.42486043 63.65510099 58.81431311 61.97448636 61.53312362\n",
      "  59.25276104 50.15104257 63.19087655 61.31053447 63.79208478 53.00133142\n",
      "  62.51054869 59.0741536  55.73606792 50.3218502  56.84403463 56.63277574\n",
      "  56.49931188 62.1271114  66.49755917 64.04083816 62.08736531 61.45318042\n",
      "  58.68264294 56.32692118 69.95709731 70.78701084 58.62944246 54.1380687\n",
      "  59.53462496 63.89227551 59.47994686 59.98631063 62.80980069 59.96547388\n",
      "  60.39492592 55.80645601 65.26290223 63.7411014  58.57350625 64.91827825\n",
      "  57.99133815 56.04949671 53.03311006 55.90990432 62.19921416]\n",
      " [64.66544015 62.9500175  59.11207862 55.34999077 54.28114763 64.24435417\n",
      "  58.28270468 55.51037963 55.38459926 60.62869057 62.88734407 55.45695418\n",
      "  61.27807113 57.97438536 60.5834553  63.06103377 53.8560144  61.10856066\n",
      "  65.07313828 56.40625234 55.61838293 56.73820092 57.47680735 61.10474837\n",
      "  56.51244611 63.12814974 59.04606983 61.05988156 60.65557632 54.71580613\n",
      "  54.84530826 60.51029558 63.73550531 58.88380491 62.07984183 61.62468903\n",
      "  59.27742371 50.30161491 63.18899462 61.3923279  63.85620347 53.06788782\n",
      "  62.58261298 59.0614467  55.82623862 50.23022127 56.89113447 56.60761262\n",
      "  56.47164114 62.05301501 66.4507128  64.02469369 62.09747747 61.44427609\n",
      "  58.75115001 56.32907886 69.90907864 70.90401613 58.58803951 54.23748373\n",
      "  59.44397575 63.98989962 59.43101115 59.92745678 62.87363564 59.82516897\n",
      "  60.5308946  55.82658125 65.27586438 63.83628688 58.66763596 64.84208298\n",
      "  57.97258148 56.07669219 53.04562621 55.99783522 62.05195627]\n",
      " [65.77361608 63.79810819 59.94220249 56.20088327 55.27652416 65.26870436\n",
      "  59.09867535 56.48629494 56.23662886 61.5538118  63.93540146 56.40925811\n",
      "  62.37487701 58.94707257 61.5114304  64.0372822  54.67517274 62.04407048\n",
      "  66.13003097 57.41324331 56.47883906 57.44320081 58.31673396 62.10006203\n",
      "  57.29828644 64.13187983 60.06844166 62.07371927 61.54829522 55.38341355\n",
      "  55.75020315 61.37528202 64.73562709 59.74033464 62.92342972 62.48993801\n",
      "  60.14579322 50.92599948 64.20254328 62.305259   64.81042806 53.84848796\n",
      "  63.46583229 59.9364811  56.62503635 51.1880781  57.74816794 57.49694742\n",
      "  57.45294027 63.12897299 67.51751979 65.02131219 63.01339258 62.39497309\n",
      "  59.64472648 57.22124939 71.09805864 71.89268352 59.57200487 55.04722441\n",
      "  60.50773475 64.87983863 60.41860962 60.93974465 63.75304388 60.901347\n",
      "  61.36335565 56.61985512 66.29103471 64.76194156 59.476385   65.93674215\n",
      "  58.94218577 56.91418732 53.89906729 56.76989626 63.17269036]\n",
      " [64.29371506 62.67723607 58.79836716 55.10268179 53.98732245 63.92255971\n",
      "  57.90750885 55.20375394 55.12227734 60.38657932 62.59344036 55.17569503\n",
      "  60.95137768 57.60235593 60.26289596 62.74338465 53.58211339 60.8177047\n",
      "  64.77042673 56.0893353  55.33386875 56.34738414 57.21771481 60.79654955\n",
      "  56.28554396 62.90198977 58.81971361 60.79357232 60.36071656 54.47133704\n",
      "  54.60700065 60.22550888 63.41714498 58.58235953 61.73488155 61.27686078\n",
      "  59.12417974 50.06985504 62.88011094 61.09157887 63.48306404 52.76406608\n",
      "  62.19254198 58.79388118 55.53171177 50.03506284 56.56250956 56.3075221\n",
      "  56.12739539 61.73741276 66.17150545 63.73707856 61.80299343 61.1099815\n",
      "  58.43465369 55.97974621 69.57407302 70.47382007 58.27330883 53.92128847\n",
      "  59.20589926 63.67194217 59.16605892 59.67911612 62.56096496 59.62714482\n",
      "  60.19555838 55.60748707 64.97167477 63.48677305 58.33972647 64.47530246\n",
      "  57.73883538 55.81361235 52.69945886 55.69743793 61.79542857]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulario:\", vocab)\n",
    "print(\"\\nSalida de Self-Attention:\\n\", self_attention_output)\n",
    "print(\"\\nSalida de Multi-Head Attention:\\n\", multi_head_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_attention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPesos de Atención: Self-Attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[1;34m(weights, title)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_attention\u001b[39m(weights, title):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "visualize_attention(self_attention_weights, \"Pesos de Atención: Self-Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
