{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Descargar el dataset\n",
    "# path = kagglehub.dataset_download(\"stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Descargar el dataset\n",
    "# path = kagglehub.dataset_download(\"stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\")\n",
    "\n",
    "# # Ruta personalizada\n",
    "# custom_path = \"dataEx\"\n",
    "\n",
    "# # Crear el directorio si no existe\n",
    "# os.makedirs(custom_path, exist_ok=True)\n",
    "\n",
    "# # Mover archivos descargados al nuevo directorio\n",
    "# for file in os.listdir(path):\n",
    "#     shutil.move(os.path.join(path, file), os.path.join(custom_path, file))\n",
    "\n",
    "# print(f\"Archivos movidos a: {custom_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Bimestral - Diseño de un Sistema Básico de Recuperación de Información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "### 1. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install TfidfVectorizer\n",
    "# %pip install pandas nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         movie_title  \\\n",
      "0  Percy Jackson & the Olympians: The Lightning T...   \n",
      "1                                        Please Give   \n",
      "2                                                 10   \n",
      "3                    12 Angry Men (Twelve Angry Men)   \n",
      "4                       20,000 Leagues Under The Sea   \n",
      "\n",
      "                                   critics_consensus  \n",
      "0  Though it may seem like just another Harry Pot...  \n",
      "1  Nicole Holofcener's newest might seem slight i...  \n",
      "2  Blake Edwards' bawdy comedy may not score a pe...  \n",
      "3  Sidney Lumet's feature debut is a superbly wri...  \n",
      "4  One of Disney's finest live-action adventures,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset de películas\n",
    "movies = pd.read_csv('dataEx/rotten_tomatoes_movies.csv')\n",
    "critic_review = pd.read_csv('dataEx/rotten_tomatoes_critic_reviews.csv')\n",
    "\n",
    "# Visualizar las primeras filas para confirmar las columnas\n",
    "# print(movies.head())\n",
    "# print(critic_review.head())\n",
    "print(movies[['movie_title', 'critics_consensus']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import pos_tag, download\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\USER\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\USER\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = os.path.expanduser('~') + '\\\\nltk_data'\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Descargar recursos\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en el dataset: Index(['rotten_tomatoes_link', 'movie_title', 'movie_info',\n",
      "       'critics_consensus', 'content_rating', 'genres', 'directors', 'authors',\n",
      "       'actors', 'original_release_date', 'streaming_release_date', 'runtime',\n",
      "       'production_company', 'tomatometer_status', 'tomatometer_rating',\n",
      "       'tomatometer_count', 'audience_status', 'audience_rating',\n",
      "       'audience_count', 'tomatometer_top_critics_count',\n",
      "       'tomatometer_fresh_critics_count', 'tomatometer_rotten_critics_count'],\n",
      "      dtype='object')\n",
      "                                   critics_consensus  \\\n",
      "0  Though it may seem like just another Harry Pot...   \n",
      "1  Nicole Holofcener's newest might seem slight i...   \n",
      "2  Blake Edwards' bawdy comedy may not score a pe...   \n",
      "3  Sidney Lumet's feature debut is a superbly wri...   \n",
      "4  One of Disney's finest live-action adventures,...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  though may seem like another harry potter knoc...  \n",
      "1  nicole holofceners new might seem slight place...  \n",
      "2  blake edward bawdy comedy may score perfect du...  \n",
      "3  sidney lumets feature debut superbly write dra...  \n",
      "4  one disney fine liveaction adventure league se...  \n",
      "Columnas disponibles en el dataset: Index(['rotten_tomatoes_link', 'critic_name', 'top_critic', 'publisher_name',\n",
      "       'review_type', 'review_score', 'review_date', 'review_content'],\n",
      "      dtype='object')\n",
      "La columna 'review' no está disponible en el dataset.\n"
     ]
    }
   ],
   "source": [
    "# Función para convertir etiquetas POS de nltk a WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Por defecto\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocesar_contenido(contenido, stopwords, signos_puntuacion):\n",
    "    # Validar contenido\n",
    "    if contenido is None or not isinstance(contenido, str):\n",
    "        return ''\n",
    "    \n",
    "    # Definir el lematizador\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Convertir texto a minúsculas\n",
    "    contenido = contenido.lower()\n",
    "\n",
    "    # Reemplazar caracteres especiales y números\n",
    "    contenido = re.sub(r'\\d+', '', contenido)  # Eliminar números\n",
    "    contenido = re.sub(r'[^\\w\\s]', '', contenido)  # Eliminar puntuación\n",
    "    contenido = re.sub(r'\\s+', ' ', contenido)  # Eliminar espacios redundantes\n",
    "\n",
    "    # Tokenización\n",
    "    contenido_tokenizado = word_tokenize(contenido)\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    tokens_sin_stopwords = [word for word in contenido_tokenizado if word not in stopwords]\n",
    "\n",
    "    # Etiquetado POS y lematización\n",
    "    tokens_pos = pos_tag(tokens_sin_stopwords)\n",
    "    tokens_lematizados = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in tokens_pos\n",
    "    ]\n",
    "\n",
    "    # Reconstruir texto lematizado\n",
    "    return ' '.join(tokens_lematizados)\n",
    "\n",
    "# Función para obtener stopwords\n",
    "def obtener_stopwords(idioma='english'):\n",
    "    return set(stopwords.words(idioma))\n",
    "\n",
    "# Preprocesar dataset\n",
    "def preprocesar_dataset(dataframe, columna_texto):\n",
    "    # Obtener stopwords\n",
    "    stop_words = obtener_stopwords('english')\n",
    "\n",
    "    # Aplicar preprocesamiento a la columna especificada\n",
    "    dataframe['processed_text'] = dataframe[columna_texto].fillna('').apply(\n",
    "        lambda x: preprocesar_contenido(x, stop_words, [])\n",
    "    )\n",
    "    return dataframe\n",
    "\n",
    "# Preprocesar archivos\n",
    "def cargar_y_preprocesar(ruta, columna_texto):\n",
    "    try:\n",
    "        # Cargar archivo\n",
    "        dataframe = pd.read_csv(ruta)\n",
    "        print(f\"Columnas disponibles en el dataset: {dataframe.columns}\")\n",
    "        \n",
    "        # Preprocesar si la columna existe\n",
    "        if columna_texto in dataframe.columns:\n",
    "            dataframe = preprocesar_dataset(dataframe, columna_texto)\n",
    "            print(dataframe[[columna_texto, 'processed_text']].head())\n",
    "        else:\n",
    "            print(f\"La columna '{columna_texto}' no está disponible en el dataset.\")\n",
    "        \n",
    "        return dataframe\n",
    "    except FileNotFoundError:\n",
    "        print(f\"El archivo '{ruta}' no fue encontrado. Verifica la ruta.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cargar y procesar datasets\n",
    "movies = cargar_y_preprocesar('dataEx/rotten_tomatoes_movies.csv', 'critics_consensus')\n",
    "critic_reviews = cargar_y_preprocesar('dataEx/rotten_tomatoes_critic_reviews.csv', 'review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construcción del Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simulación de Consultas: El sistema debe mostrar un listado de las películas más relevantes para cada consulta, ordenadas según su relevancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
