{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Descargar el dataset\n",
    "# path = kagglehub.dataset_download(\"stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Descargar el dataset\n",
    "# path = kagglehub.dataset_download(\"stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\")\n",
    "\n",
    "# # Ruta personalizada\n",
    "# custom_path = \"dataEx\"\n",
    "\n",
    "# # Crear el directorio si no existe\n",
    "# os.makedirs(custom_path, exist_ok=True)\n",
    "\n",
    "# # Mover archivos descargados al nuevo directorio\n",
    "# for file in os.listdir(path):\n",
    "#     shutil.move(os.path.join(path, file), os.path.join(custom_path, file))\n",
    "\n",
    "# print(f\"Archivos movidos a: {custom_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Bimestral - Diseño de un Sistema Básico de Recuperación de Información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "### 1. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install TfidfVectorizer\n",
    "# %pip install pandas nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset de películas\n",
    "movies = pd.read_csv('dataEx/rotten_tomatoes_movies.csv')\n",
    "critic_review = pd.read_csv('dataEx/rotten_tomatoes_critic_reviews.csv')\n",
    "\n",
    "# Visualizar las primeras filas para confirmar las columnas\n",
    "# print(movies.head())\n",
    "# print(critic_review.head())\n",
    "print(movies[['movie_title', 'critics_consensus']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import pos_tag, download\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = os.path.expanduser('~') + '\\\\nltk_data'\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Descargar recursos\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para convertir etiquetas POS de nltk a WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Por defecto\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocesar_contenido(contenido, stopwords, signos_puntuacion):\n",
    "    # Validar contenido\n",
    "    if contenido is None or not isinstance(contenido, str):\n",
    "        return ''\n",
    "    \n",
    "    # Definir el lematizador\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Convertir texto a minúsculas\n",
    "    contenido = contenido.lower()\n",
    "\n",
    "    # Reemplazar caracteres especiales y números\n",
    "    contenido = re.sub(r'\\d+', '', contenido)  # Eliminar números\n",
    "    contenido = re.sub(r'[^\\w\\s]', '', contenido)  # Eliminar puntuación\n",
    "    contenido = re.sub(r'\\s+', ' ', contenido)  # Eliminar espacios redundantes\n",
    "\n",
    "    # Tokenización\n",
    "    contenido_tokenizado = word_tokenize(contenido)\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    tokens_sin_stopwords = [word for word in contenido_tokenizado if word not in stopwords]\n",
    "\n",
    "    # Etiquetado POS y lematización\n",
    "    tokens_pos = pos_tag(tokens_sin_stopwords)\n",
    "    tokens_lematizados = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in tokens_pos\n",
    "    ]\n",
    "\n",
    "    # Reconstruir texto lematizado\n",
    "    return ' '.join(tokens_lematizados)\n",
    "\n",
    "# Función para obtener stopwords\n",
    "def obtener_stopwords(idioma='english'):\n",
    "    return set(stopwords.words(idioma))\n",
    "\n",
    "# Preprocesar dataset\n",
    "def preprocesar_dataset(dataframe, columna_texto):\n",
    "    # Obtener stopwords\n",
    "    stop_words = obtener_stopwords('english')\n",
    "\n",
    "    # Aplicar preprocesamiento a la columna especificada\n",
    "    dataframe['processed_text'] = dataframe[columna_texto].fillna('').apply(\n",
    "        lambda x: preprocesar_contenido(x, stop_words, [])\n",
    "    )\n",
    "    return dataframe\n",
    "\n",
    "# Preprocesar archivos\n",
    "def cargar_y_preprocesar(ruta, columna_texto):\n",
    "    try:\n",
    "        # Cargar archivo\n",
    "        dataframe = pd.read_csv(ruta)\n",
    "        print(f\"Columnas disponibles en el dataset: {dataframe.columns}\")\n",
    "        \n",
    "        # Preprocesar si la columna existe\n",
    "        if columna_texto in dataframe.columns:\n",
    "            dataframe = preprocesar_dataset(dataframe, columna_texto)\n",
    "            print(dataframe[[columna_texto, 'processed_text']].head())\n",
    "        else:\n",
    "            print(f\"La columna '{columna_texto}' no está disponible en el dataset.\")\n",
    "        \n",
    "        return dataframe\n",
    "    except FileNotFoundError:\n",
    "        print(f\"El archivo '{ruta}' no fue encontrado. Verifica la ruta.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cargar y procesar datasets\n",
    "movies = cargar_y_preprocesar('dataEx/rotten_tomatoes_movies.csv', 'critics_consensus')\n",
    "critic_reviews = cargar_y_preprocesar('dataEx/rotten_tomatoes_critic_reviews.csv', 'review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construcción del Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simulación de Consultas: El sistema debe mostrar un listado de las películas más relevantes para cada consulta, ordenadas según su relevancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
